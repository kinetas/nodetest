from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import evaluate

# ğŸ§ª ë°ì´í„° ë¡œë“œ (CSV or JSONL ì„ íƒ)
dataset = load_dataset("csv", data_files="intent_data.csv")  # ë˜ëŠ” "csv"

# ğŸ§  ë¼ë²¨ ì¸ì½”ë”©
label_list = ["GENERAL", "SPECIFIC"]
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for i, label in enumerate(label_list)}

def encode_labels(example):
    example["label"] = label2id[example["label"]]
    return example

dataset = dataset.map(encode_labels)

# âœ‚ï¸ í† í¬ë‚˜ì´ì§•
model_name = "monologg/koelectra-small-v3-discriminator"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
    return tokenizer(example["text"], padding="max_length", truncation=True)

dataset = dataset.map(tokenize)

# ğŸ“¦ ëª¨ë¸ ë¡œë”©
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(label_list),
    id2label=id2label,
    label2id=label2id
)

# ğŸ‹ï¸â€â™‚ï¸ Trainer ì„¤ì •
training_args = TrainingArguments(
    output_dir="./intent_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    save_total_limit=1,
    load_best_model_at_end=True
)

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return metric.compute(predictions=preds, references=labels)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["train"],
    compute_metrics=compute_metrics
)

# ğŸš€ í•™ìŠµ ì‹œì‘
trainer.train()

# ğŸ’¾ ì €ì¥
trainer.save_model("./intent_model")
tokenizer.save_pretrained("./intent_model")
model.save_pretrained("./intent_model")