# from datasets import load_dataset, DatasetDict
# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
# import numpy as np
# import evaluate

# # ğŸ§ª ë°ì´í„° ë¡œë“œ (CSV or JSONL ì„ íƒ)
# dataset = load_dataset("csv", data_files="intent_data.csv")  # ë˜ëŠ” "csv"

# # ğŸ§  ë¼ë²¨ ì¸ì½”ë”©
# label_list = ["GENERAL", "SPECIFIC"]
# label2id = {label: i for i, label in enumerate(label_list)}
# id2label = {i: label for i, label in enumerate(label_list)}

# def encode_labels(example):
#     example["label"] = label2id[example["label"]]
#     return example

# dataset = dataset.map(encode_labels)

# # âœ‚ï¸ í† í¬ë‚˜ì´ì§•
# model_name = "monologg/koelectra-small-v3-discriminator"
# tokenizer = AutoTokenizer.from_pretrained(model_name)

# def tokenize(example):
#     return tokenizer(example["text"], padding="max_length", truncation=True)

# dataset = dataset.map(tokenize)

# # ğŸ“¦ ëª¨ë¸ ë¡œë”©
# model = AutoModelForSequenceClassification.from_pretrained(
#     model_name,
#     num_labels=len(label_list),
#     id2label=id2label,
#     label2id=label2id
# )

# # ğŸ‹ï¸â€â™‚ï¸ Trainer ì„¤ì •
# training_args = TrainingArguments(
#     output_dir="./intent_model",
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     num_train_epochs=5,
#     per_device_train_batch_size=8,
#     save_total_limit=1,
#     load_best_model_at_end=True
# )

# metric = evaluate.load("accuracy")

# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     preds = np.argmax(logits, axis=-1)
#     return metric.compute(predictions=preds, references=labels)

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=dataset["train"],
#     eval_dataset=dataset["train"],
#     compute_metrics=compute_metrics
# )

# # ğŸš€ í•™ìŠµ ì‹œì‘
# trainer.train()

# # ğŸ’¾ ì €ì¥
# trainer.save_model("./intent_model")
# tokenizer.save_pretrained("./intent_model")
# model.save_pretrained("./intent_model")

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import evaluate
import torch
from torch import nn
from sklearn.utils.class_weight import compute_class_weight
import os

# ğŸ§ª ë°ì´í„° ë¡œë“œ

dataset = load_dataset("csv", data_files="intent_data.csv")["train"]

# ğŸ§  ë¼ë²¨ ì¸ì½”ë”©
label_list = ["GENERAL", "SPECIFIC"]
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for i, label in enumerate(label_list)}

def encode_labels(example):
    example["label"] = label2id[example["label"]]
    return example

dataset = dataset.map(encode_labels)

# âœ‚ï¸ í† í¬ë‚˜ì´ì§•
model_name = "monologg/koelectra-small-v3-discriminator"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
    return tokenizer(example["text"], padding="max_length", truncation=True)

dataset = dataset.map(tokenize)

# âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
labels = [example["label"] for example in dataset]
class_weights = compute_class_weight(class_weight="balanced", classes=np.unique(labels), y=labels)
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)

# ğŸ“¦ ëª¨ë¸ ë¡œë”©
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(label_list),
    id2label=id2label,
    label2id=label2id
)

# ğŸ¯ loss_fn ìˆ˜ì •í•œ Trainer ë§Œë“¤ê¸°
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

# Trainer ì„¤ì •
training_args = TrainingArguments(
    output_dir="./intent_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    save_total_limit=1,
    load_best_model_at_end=True
)

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return metric.compute(predictions=preds, references=labels)

# ğŸ‹ï¸â€â™‚ï¸ í•™ìŠµ
trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    eval_dataset=dataset,
    compute_metrics=compute_metrics
)
SAVE_PATH = "/app/intent_model"
os.makedirs(SAVE_PATH, exist_ok=True)

trainer.train()
trainer.save_model(SAVE_PATH)
tokenizer.save_pretrained(SAVE_PATH)
# ğŸ’¾ ëª¨ë¸ ì €ì¥
trainer.save_model("./intent_model")
tokenizer.save_pretrained("./intent_model")
model.save_pretrained("./intent_model")
