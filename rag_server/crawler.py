# import requests
# from bs4 import BeautifulSoup
# import json
# import uuid
# import time

# # User-Agent ê°•í™” (ìµœì‹  í¬ë¡¬ í—¤ë”ì²˜ëŸ¼)
# HEADERS = {
#     "User-Agent": (
#         "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
#         "AppleWebKit/537.36 (KHTML, like Gecko) "
#         "Chrome/123.0.0.0 Safari/537.36"
#     )
# }

# # ë¸”ë¡œê·¸ ë§í¬ ì¶”ì¶œ í•¨ìˆ˜
# def get_blog_links(keyword, max_links=3):
#     query = keyword.replace(" ", "+")
#     url = f"https://search.naver.com/search.naver?where=view&query={query}&sm=tab_opt"
#     res = requests.get(url, headers=HEADERS)
#     print(f"ğŸ” [{keyword}] ê²€ìƒ‰ í˜ì´ì§€ ì‘ë‹µì½”ë“œ: {res.status_code}")

#     soup = BeautifulSoup(res.text, "html.parser")

#     links = []
#     for a in soup.select("a.api_txt_lines.total_tit"):
#         href = a.get("href")
#         if href.startswith("https://blog.naver.com"):
#             print("âœ… ë¸”ë¡œê·¸ ë§í¬ ë°œê²¬:", href)
#             links.append(href)
#         if len(links) >= max_links:
#             break

#     print(f"ğŸ”— ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ë§í¬ ìˆ˜: {len(links)}")
#     return links

# # ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜
# def crawl_naver_blog(url):
#     try:
#         res = requests.get(url, headers=HEADERS)
#         soup = BeautifulSoup(res.text, "html.parser")

#         iframe = soup.select_one("iframe#mainFrame")
#         if not iframe:
#             print("â— iframe ì—†ìŒ:", url)
#             return ""

#         iframe_url = "https://blog.naver.com" + iframe["src"]
#         res2 = requests.get(iframe_url, headers=HEADERS)
#         soup2 = BeautifulSoup(res2.text, "html.parser")

#         content_div = (
#             soup2.select_one("div.se-main-container") or
#             soup2.select_one("div#postViewArea")
#         )
#         if content_div:
#             return content_div.get_text("\n", strip=True)

#         print("â— ë³¸ë¬¸ div ì—†ìŒ:", iframe_url)
#         return ""

#     except Exception as e:
#         print("âŒ ì˜ˆì™¸ ë°œìƒ:", e)
#         return ""

# # ìˆ˜ì§‘ í‚¤ì›Œë“œ
# keywords = [
#     ("ë¯¸ë¼í´ ëª¨ë‹ ë£¨í‹´", "ìê¸°ê´€ë¦¬"),
#     ("ìê¸°ê°œë°œ ë£¨í‹´", "ìê¸°ê°œë°œ"),
#     ("ìš´ë™ ë£¨í‹´", "ìš´ë™"),
#     ("ìš”ë¦¬ ë£¨í‹´", "ìƒí™œìŠµê´€")
# ]

# collected = []

# for keyword, category in keywords:
#     print(f"\nğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹œì‘...")
#     blog_links = get_blog_links(keyword)

#     for link in blog_links:
#         print("â¡ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„:", link)
#         content = crawl_naver_blog(link)
#         print("ğŸ“„ ë³¸ë¬¸ ê¸¸ì´:", len(content))
#         time.sleep(3)

#         if content:
#             collected.append({
#                 "id": str(uuid.uuid4()),
#                 "document": content[:2000],
#                 "metadata": {
#                     "tag": keyword,
#                     "category": category,
#                     "source": link
#                 }
#             })

# # ì €ì¥
# with open("blog_data.json", "w", encoding="utf-8") as f:
#     json.dump(collected, f, ensure_ascii=False, indent=2)

# print(f"\nâœ… {len(collected)}ê°œì˜ ë¸”ë¡œê·¸ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ â†’ blog_data.json")



# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.chrome.service import Service
# from webdriver_manager.chrome import ChromeDriverManager
# from bs4 import BeautifulSoup
# import time, json, uuid

# # ì…€ë ˆë‹ˆì›€ ì˜µì…˜ ì„¤ì •
# options = Options()
# options.add_argument('--headless')  # GUI ì—†ì´ ì‹¤í–‰
# options.add_argument('--no-sandbox')
# options.add_argument('--disable-dev-shm-usage')
# options.add_argument('--disable-gpu')
# options.add_argument('--window-size=1920x1080')

# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# keywords = ["ë¯¸ë¼í´ ëª¨ë‹ ë£¨í‹´", "ìê¸°ê°œë°œ ë£¨í‹´", "ìš´ë™ ë£¨í‹´", "ìš”ë¦¬ ë£¨í‹´"]
# collected = []

# for keyword in keywords:
#     print(f"ğŸ” '{keyword}' ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹œì‘...")
#     search_url = f"https://section.blog.naver.com/Search/Post.naver?keyword={keyword}"
#     driver.get(search_url)
#     time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

#     soup = BeautifulSoup(driver.page_source, "html.parser")
#     cards = soup.select("div.desc > a")

#     links = []
#     for a in cards:
#         href = a.get("href")
#         if href and "blog.naver.com" in href:
#             links.append(href)

#     print(f"ğŸ”— ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ë§í¬ ìˆ˜: {len(links)}")

#     for link in links:
#         driver.get(link)
#         time.sleep(2)
#         blog_soup = BeautifulSoup(driver.page_source, "html.parser")
#         content = blog_soup.get_text(separator="\n").strip()
#         if content:
#             collected.append({
#                 "id": str(uuid.uuid4()),
#                 "document": content[:2000],
#                 "metadata": {
#                     "tag": keyword,
#                     "category": "ë£¨í‹´",
#                     "source": link
#                 }
#             })

# driver.quit()

# with open("blog_data.json", "w", encoding="utf-8") as f:
#     json.dump(collected, f, ensure_ascii=False, indent=2)

# print(f"âœ… {len(collected)}ê°œì˜ ë¸”ë¡œê·¸ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ â†’ blog_data.json")

import time
import uuid
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# âœ… ë¸”ë¡œê·¸ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜
def extract_blog_content(driver, url):
    try:
        driver.get(url)
        time.sleep(2)

        driver.switch_to.frame("mainFrame")
        time.sleep(1)

        try:
            content = driver.find_element(By.CSS_SELECTOR, "div.se-main-container").text.strip()
        except:
            content = driver.find_element(By.CSS_SELECTOR, "div#postViewArea").text.strip()

        driver.switch_to.default_content()
        return content
    except Exception as e:
        print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

# âœ… ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
keywords = [
    ("ë¯¸ë¼í´ ëª¨ë‹ ë£¨í‹´", "ë£¨í‹´"),
    ("ìê¸°ê°œë°œ ë£¨í‹´", "ë£¨í‹´"),
    ("ìš´ë™ ë£¨í‹´", "ë£¨í‹´"),
    ("ìš”ë¦¬ ë£¨í‹´", "ë£¨í‹´")
]

# âœ… ì…€ë ˆë‹ˆì›€ ì„¤ì •
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('--disable-gpu')
options.add_argument('--window-size=1920x1080')

# âœ… ë“œë¼ì´ë²„ ì‹¤í–‰
print("ğŸ”„ í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰ ì¤‘...")
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

collected = []

for keyword, category in keywords:
    print(f"\nğŸ” '{keyword}' ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹œì‘...")
    search_url = f"https://section.blog.naver.com/Search/Post.naver?keyword={keyword}"
    driver.get(search_url)

    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.desc > a"))
        )
        cards = driver.find_elements(By.CSS_SELECTOR, "div.desc > a")
        links = [card.get_attribute("href") for card in cards[:3]]
        print(f"ğŸ”— ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ë§í¬ ìˆ˜: {len(links)}")

        for link in links:
            content = extract_blog_content(driver, link)
            if content:
                collected.append({
                    "id": str(uuid.uuid4()),
                    "document": content[:2000],
                    "metadata": {
                        "tag": keyword,
                        "category": category,
                        "source": link
                    }
                })
            time.sleep(3)  # ë”œë ˆì´ ì¶”ê°€

    except Exception as e:
        print(f"âš  ë¸”ë¡œê·¸ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

# âœ… ì¢…ë£Œ ë° ì €ì¥
driver.quit()

with open("blog_data.json", "w", encoding="utf-8") as f:
    json.dump(collected, f, ensure_ascii=False, indent=2)

print(f"\nâœ… {len(collected)}ê°œì˜ ë¸”ë¡œê·¸ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ â†’ blog_data.json")