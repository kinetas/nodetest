import requests
from bs4 import BeautifulSoup
import json
import uuid
import time

# User-Agent ê°•í™” (ìµœì‹  í¬ë¡¬ í—¤ë”ì²˜ëŸ¼)
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36"
    )
}

# ë¸”ë¡œê·¸ ë§í¬ ì¶”ì¶œ í•¨ìˆ˜
def get_blog_links(keyword, max_links=3):
    query = keyword.replace(" ", "+")
    url = f"https://search.naver.com/search.naver?where=view&query={query}&sm=tab_opt"
    res = requests.get(url, headers=HEADERS)
    print(f"ğŸ” [{keyword}] ê²€ìƒ‰ í˜ì´ì§€ ì‘ë‹µì½”ë“œ: {res.status_code}")

    soup = BeautifulSoup(res.text, "html.parser")

    links = []
    for a in soup.select("a.api_txt_lines.total_tit"):
        href = a.get("href")
        if href.startswith("https://blog.naver.com"):
            print("âœ… ë¸”ë¡œê·¸ ë§í¬ ë°œê²¬:", href)
            links.append(href)
        if len(links) >= max_links:
            break

    print(f"ğŸ”— ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ë§í¬ ìˆ˜: {len(links)}")
    return links

# ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜
def crawl_naver_blog(url):
    try:
        res = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(res.text, "html.parser")

        iframe = soup.select_one("iframe#mainFrame")
        if not iframe:
            print("â— iframe ì—†ìŒ:", url)
            return ""

        iframe_url = "https://blog.naver.com" + iframe["src"]
        res2 = requests.get(iframe_url, headers=HEADERS)
        soup2 = BeautifulSoup(res2.text, "html.parser")

        content_div = (
            soup2.select_one("div.se-main-container") or
            soup2.select_one("div#postViewArea")
        )
        if content_div:
            return content_div.get_text("\n", strip=True)

        print("â— ë³¸ë¬¸ div ì—†ìŒ:", iframe_url)
        return ""

    except Exception as e:
        print("âŒ ì˜ˆì™¸ ë°œìƒ:", e)
        return ""

# ìˆ˜ì§‘ í‚¤ì›Œë“œ
keywords = [
    ("ë¯¸ë¼í´ ëª¨ë‹ ë£¨í‹´", "ìê¸°ê´€ë¦¬"),
    ("ìê¸°ê°œë°œ ë£¨í‹´", "ìê¸°ê°œë°œ"),
    ("ìš´ë™ ë£¨í‹´", "ìš´ë™"),
    ("ìš”ë¦¬ ë£¨í‹´", "ìƒí™œìŠµê´€")
]

collected = []

for keyword, category in keywords:
    print(f"\nğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹œì‘...")
    blog_links = get_blog_links(keyword)

    for link in blog_links:
        print("â¡ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„:", link)
        content = crawl_naver_blog(link)
        print("ğŸ“„ ë³¸ë¬¸ ê¸¸ì´:", len(content))
        time.sleep(3)

        if content:
            collected.append({
                "id": str(uuid.uuid4()),
                "document": content[:2000],
                "metadata": {
                    "tag": keyword,
                    "category": category,
                    "source": link
                }
            })

# ì €ì¥
with open("blog_data.json", "w", encoding="utf-8") as f:
    json.dump(collected, f, ensure_ascii=False, indent=2)

print(f"\nâœ… {len(collected)}ê°œì˜ ë¸”ë¡œê·¸ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ â†’ blog_data.json")
