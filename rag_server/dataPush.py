import hashlib
import os
from langchain_community.document_loaders import TextLoader
from langchain_community.text_splitter import CharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma

# í´ë” ë° DB ì„¤ì •
docs_folder = "documents"
persist_directory = "db"

# Ollama Embedding ì´ˆê¸°í™”
embedding = OllamaEmbeddings(base_url="http://ollama:11434", model="llama3")

# DB ë¡œë“œ ë˜ëŠ” ìƒì„±
db = Chroma(persist_directory=persist_directory, embedding_function=embedding)

# í•´ì‹œê°’ ìƒì„± í•¨ìˆ˜ (ì¤‘ë³µ ë°©ì§€)
def get_doc_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

existing_hashes = {get_doc_hash(doc) for doc in db.get()['documents']}

# ìƒˆ ë¬¸ì„œ ë¡œë“œ
new_docs = []
for filename in os.listdir(docs_folder):
    if filename.endswith(".txt"):
        filepath = os.path.join(docs_folder, filename)
        loader = TextLoader(filepath)
        docs = loader.load()

        splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
        split_docs = splitter.split_documents(docs)

        for doc in split_docs:
            doc_hash = get_doc_hash(doc.page_content)
            if doc_hash not in existing_hashes:
                new_docs.append(doc)
                existing_hashes.add(doc_hash)

# DBì— ì¶”ê°€
if new_docs:
    db.add_documents(new_docs)
    print(f"âœ… {len(new_docs)}ê°œì˜ ìƒˆë¡œìš´ ë¬¸ì„œê°€ DBì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("ğŸš¨ ì¶”ê°€í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
