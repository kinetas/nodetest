# import hashlib
# import os
# from langchain_community.document_loaders import TextLoader
# from langchain.text_splitter import CharacterTextSplitter
# from langchain_ollama import OllamaEmbeddings
# from langchain_chroma import Chroma

# # í´ë” ë° DB ì„¤ì •
# docs_folder = "documents"
# persist_directory = "db"

# # Ollama Embedding ì´ˆê¸°í™”
# embedding = OllamaEmbeddings(base_url="http://ollama:11434", model="llama3")

# # DB ë¡œë“œ ë˜ëŠ” ìƒì„±
# db = Chroma(persist_directory="/chroma/chroma", embedding_function=embedding)

# # í•´ì‹œê°’ ìƒì„± í•¨ìˆ˜ (ì¤‘ë³µ ë°©ì§€)
# def get_doc_hash(text):
#     return hashlib.md5(text.encode('utf-8')).hexdigest()

# existing_hashes = {get_doc_hash(doc) for doc in db.get()['documents']}

# # ìƒˆ ë¬¸ì„œ ë¡œë“œ
# new_docs = []
# for filename in os.listdir(docs_folder):
#     if filename.endswith(".txt"):
#         filepath = os.path.join(docs_folder, filename)
#         loader = TextLoader(filepath)
#         docs = loader.load()

#         splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
#         split_docs = splitter.split_documents(docs)

#         for doc in split_docs:
#             doc_hash = get_doc_hash(doc.page_content)
#             if doc_hash not in existing_hashes:
#                 new_docs.append(doc)
#                 existing_hashes.add(doc_hash)

# # DBì— ì¶”ê°€
# if new_docs:
#     db.add_documents(new_docs)
#     print(f"âœ… {len(new_docs)}ê°œì˜ ìƒˆë¡œìš´ ë¬¸ì„œê°€ DBì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
# else:
#     print("ğŸš¨ ì¶”ê°€í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
import json
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain_ollama import OllamaEmbeddings
import hashlib
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_chroma import Chroma

# ê²½ë¡œ ì„¤ì •
json_file = "documents/data.json"  # ğŸ‘ˆ ì—¬ê¸°ì— JSON ì €ì¥
persist_directory = "/chroma/chroma"

# ì„ë² ë”© ì´ˆê¸°í™”
embedding = OllamaEmbeddings(base_url="http://ollama:11434", model="llama3")
db = Chroma(persist_directory=persist_directory, embedding_function=embedding)

db._collection.delete()

# JSON ë¶ˆëŸ¬ì˜¤ê¸°
with open(json_file, "r", encoding="utf-8") as f:
    data = json.load(f)

# Document ê°ì²´ë¡œ ë³€í™˜
docs = [
    Document(page_content=item["document"], metadata=item["metadata"])
    for item in data
]

# DBì— ì¶”ê°€
db.add_documents(docs)
print(f"âœ… {len(docs)}ê°œì˜ ë¬¸ì„œê°€ DBì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

