from fastapi import FastAPI
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from dotenv import load_dotenv
import os, requests, json, re, time
from bs4 import BeautifulSoup
from langchain_community.vectorstores import Chroma
# from langchain_ollama import OllamaEmbeddings
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
import random

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"

# âœ… FastAPI ì´ˆê¸°í™”
app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/ai")
def serve_index():
    return FileResponse("static/index.html")

# âœ… ë²¡í„° DB ë° ì„ë² ë”© ì˜¬ë¼ë§ˆë²„ì „
# embedding = OllamaEmbeddings(base_url="http://ollama:11434", model="nomic-embed-text")
# db = Chroma(persist_directory="/chroma/chroma", embedding_function=embedding)

embedding = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)
db = Chroma(persist_directory="/chroma/chroma", embedding_function=embedding)

# âœ… ë¸”ë¡œê·¸ ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_naver_blog(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        time.sleep(3) 
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")

        iframe = soup.select_one("iframe#mainFrame")
        if iframe:
            iframe_url = "https://blog.naver.com" + iframe["src"]
            res2 = requests.get(iframe_url, headers=headers, timeout=10)
            soup2 = BeautifulSoup(res2.text, "html.parser")
            content_div = soup2.select_one("div.se-main-container")
            if content_div:
                return content_div.get_text("\n", strip=True)
        else:
            content_div = soup.select_one("div.se-main-container")
            if content_div:
                return content_div.get_text("\n", strip=True)
    except Exception as e:
        print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨:", e)
    return None

# âœ… API ëª¨ë¸
class ChatRequest(BaseModel):
    category: str

@app.post("/ai/recommend")
async def recommend(req: ChatRequest):
    start_time = time.time()
    query = f"{req.category} ê´€ë ¨í•´ì„œ ì˜¤ëŠ˜ í•´ë³¼ ë§Œí•œ ë¯¸ì…˜ í•˜ë‚˜ ì¶”ì²œí•´ì¤˜."

    # ğŸ” RAG ê²€ìƒ‰
    docs_with_scores = db.similarity_search_with_score(query, k=10)
    print("ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼:")
    for i, (doc, score) in enumerate(docs_with_scores):
        content = doc.page_content or "(âš ï¸ ë‚´ìš© ì—†ìŒ)"
        try:
            preview = content[:100].replace('\n', ' ')
        except Exception as e:
            preview = f"(âš ï¸ ì¶œë ¥ ì‹¤íŒ¨: {e})"
        print(f"  {i+1}. ì ìˆ˜: {score:.4f}")
        print(f"     ìš”ì•½: {preview}")
        print(f"     ì¶œì²˜: {doc.metadata.get('source', '(ì—†ìŒ)')}")
    filtered_docs_with_scores = [(doc, score) for doc, score in docs_with_scores if score < 1.2]

    if not filtered_docs_with_scores:
        # âœ… fallback - CoT ë°©ì‹
        prompt = (
            "ë„ˆëŠ” ë¯¸ì…˜ ì¶”ì²œ AIì•¼. ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , JSON ì™¸ì—ëŠ” ì•„ë¬´ ê²ƒë„ ì¶œë ¥í•˜ì§€ ë§ˆ.\n"
            'message í•­ëª©ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì— ë§ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ê·¸ ì¹´í…Œê³ ë¦¬ê°€ ì–´ë–¤ íŠ¹ì§•ê³¼ íš¨ê³¼ê°€ ìˆëŠ”ì§€ ì•Œë ¤ì£¼ê³  ê·¸ì— ë”°ë¥¸ ë¯¸ì…˜ì„ ì¶”ì²œí•´ì£¼ê³  ê·¸ê²Œ ì™œ ì¹´í…Œê³ ë¦¬ì˜ íŠ¹ì§•ì´ë‚˜ íš¨ê³¼ì™€ ê´€ë ¨ìˆëŠ”ì§€ ê·¼ê±°ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ë¶€ë“œëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë¯¸ì…˜ì¶”ì²œì¤˜."\n\n'
            "category í•­ëª©ì€ í•´ë‹¹ ë¯¸ì…˜ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ í•˜ë‚˜ë¡œ ìš”ì•½í•´ì„œ ë„£ì–´. (ì˜ˆ: ìš´ë™, ê°ì •ê´€ë¦¬, ìê¸°ê´€ë¦¬, ì§‘ì¤‘ ë“±)\n\n"
            "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´:\n"
            '{\n'
            '  "message": "ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œëœ ë¯¸ì…˜ì¶”ì²œ",\n'
            '  "category": "ì¹´í…Œê³ ë¦¬"\n'
            "}\n\n"
            f"ì‚¬ìš©ì ìš”ì²­: {query}"
        )
    else:
        # âœ… ì²« ë¬¸ì„œì—ì„œ ë³¸ë¬¸ í¬ë¡¤ë§
        # 2. ì ìˆ˜ ì°¨ì´ê°€ ê±°ì˜ ì—†ë‹¤ë©´ ëœë¤ ì„ íƒ
        if len(filtered_docs_with_scores) >= 2 and abs(filtered_docs_with_scores[0][1] - filtered_docs_with_scores[1][1]) < 0.03:
            selected_doc = random.choice(filtered_docs_with_scores)[0]
        else:
            selected_doc = filtered_docs_with_scores[0][0]  # ìœ ì‚¬ë„ 1ë“± ë¬¸ì„œ
        url = selected_doc.metadata.get("source")
        blog_text = crawl_naver_blog(url) or ""
        print(f"\nğŸŒ ì„ íƒëœ ë¬¸ì„œ URL: {url}")

        blog_text = crawl_naver_blog(url) or ""
        print(f"ğŸ“„ í¬ë¡¤ë§ëœ ë¸”ë¡œê·¸ ë³¸ë¬¸ ê¸¸ì´: {len(blog_text)}ì")
        print(f"ğŸ“„ ë³¸ë¬¸ ì¼ë¶€:\n{blog_text[:500]}...\n")  # â† ì´ê²Œ í•µì‹¬!

        prompt = (
            "ë„ˆëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¯¸ì…˜ì„ ì¶”ì²œí•˜ëŠ” AIì•¼.\n"
            "ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , JSON ì™¸ì—ëŠ” ì•„ë¬´ ê²ƒë„ ì¶œë ¥í•˜ì§€ ë§ˆ.\n\n"
            "message í•­ëª©ì€ ì°¸ê³  ë¸”ë¡œê·¸ ë³¸ë¬¸ì˜ ë‚´ìš©ì„ ë³´ê³  ê·¸ê±¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³  ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨í•´ì„œ ì–´ë–¤ ê´€ë ¨ì´ ìˆê³ , ì–´ë–¤ ì¢…ë¥˜ê°€ ìˆê³ , ì–´ë–¤ íš¨ê³¼ë‚˜ ì˜í–¥ì´ ìˆëŠ”ì§€ ë§í•´ì•¼ í•˜ë©°, ì´ë¥¼ 4~5ì¤„ ì •ë„ ë˜ë„ë¡ ë°˜ë“œì‹œ ê¸¸ê³  ìì—°ìŠ¤ëŸ½ê³  ë¶€ë“œëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë¯¸ì…˜ì„ ì¶”ì²œí•´ì•¼í•´. ë˜í•œ ì™œ ì¹´í…Œê³ ë¦¬ê°€ í•´ë‹¹ ë‹µë³€ì´ ì–´ë–¤ ê´€ë ¨ì´ ìˆëŠ”ì§€ ë¶„ì„ í›„ ë§í•˜ëŠ” ê²ƒë„ ìˆì–´ì•¼í•´.\n"
            # 'ì˜ˆì‹œ: "ì±…ìƒ ì •ë¦¬ë¥¼ í•´ë³´ëŠ” ê±´ ì–´ë•Œìš”? ë§ˆìŒë„ í•¨ê»˜ ì •ë¦¬ë  ê±°ì˜ˆìš”."\n\n'
            "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´:\n"
            '{\n'
            '  "message": "ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ëœ ë¯¸ì…˜ì¶”ì²œ",\n'
            '  "category": "ì¹´í…Œê³ ë¦¬"\n'
            "}\n\n"
            f"ì°¸ê³  ë¸”ë¡œê·¸ ë³¸ë¬¸:\n{blog_text[:3000]}\n\n"
            f"ì‚¬ìš©ì ìš”ì²­: {query}"
        )

    # âœ… Groq API í˜¸ì¶œ
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    body = {
        "model": "llama3-8b-8192",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7
    }

    try:
        response = requests.post(GROQ_API_URL, headers=headers, json=body)
        result = response.json()
        content = result["choices"][0]["message"]["content"]

        json_match = re.search(r"\{.*\}", content, re.DOTALL)
        if not json_match:
            raise ValueError("ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        parsed = json.loads(json_match.group(0).replace("'", '"'))
        parsed["response_time_sec"] = round(time.time() - start_time, 2)
        return parsed

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# âœ… ë””ë²„ê¹…ìš© ë¬¸ì„œ í™•ì¸ìš© API
@app.get("/ai/documents")
async def get_documents():
    try:
        data = db.get()
        documents_info = []
        for i in range(len(data["ids"])):
            doc = {
                "id": data["ids"][i],
                "document": data["documents"][i],
                "metadata": data["metadatas"][i]
            }
            documents_info.append(doc)
        return JSONResponse(content={"documents": documents_info})
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})