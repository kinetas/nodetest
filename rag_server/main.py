# from fastapi import FastAPI
# from fastapi.responses import JSONResponse, FileResponse
# from fastapi.staticfiles import StaticFiles
# from pydantic import BaseModel
# from langchain_ollama import OllamaLLM
# from langchain_chroma import Chroma
# from langchain_ollama import OllamaEmbeddings
# from langchain.chains import RetrievalQA


# app = FastAPI()

# embedding = OllamaEmbeddings(base_url="http://ollama:11434", model="llama3")
# db = Chroma(persist_directory="/chroma/chroma", embedding_function=embedding)  # ê³µìœ  ë³¼ë¥¨ ê²½ë¡œ
# retriever = db.as_retriever()

# llm = OllamaLLM(base_url="http://ollama:11434", model="llama3")  # Ollama ëª¨ë¸ ì‚¬ìš©

# qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# class RAGRequest(BaseModel):
#     category: str

# @app.post("/recommend")
# async def recommend(req: RAGRequest):
#     query = f"{req.category} ì˜¤ëŠ˜ í•´ë³¼ ë§Œí•œ ë¯¸ì…˜ 2ê°€ì§€ ì¶”ì²œí•´ì¤˜. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì§§ê²Œ ë§í•´ì¤˜."
#     response = qa.run(query)
#     return {"message": response}

# @app.get("/documents")
# async def get_documents():
#     try:
#         data = db.get()
#         documents_info = []
#         for i in range(len(data['ids'])):
#             doc = {
#                 "id": data['ids'][i],
#                 "document": data['documents'][i],
#                 "metadata": data['metadatas'][i]
#             }
#             documents_info.append(doc)
#         return JSONResponse(content={"documents": documents_info})
#     except Exception as e:
#         return JSONResponse(status_code=500, content={"error": str(e)})

# # ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •
# app.mount("/static", StaticFiles(directory="static"), name="static")

# @app.get("/")
# def serve_index():
#     return FileResponse("static/index.html")
# from fastapi import FastAPI
# from fastapi.responses import JSONResponse, FileResponse
# from fastapi.staticfiles import StaticFiles
# from pydantic import BaseModel
# import os
# import requests
# from dotenv import load_dotenv
# load_dotenv()

# app = FastAPI()

# class RAGRequest(BaseModel):
#     category: str

# GROQ_API_KEY = os.getenv("GROQ_API_KEY")
# GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"

# @app.post("/recommend")
# async def recommend(req: RAGRequest):
#     prompt = f"{req.category} ì˜¤ëŠ˜ í•´ë³¼ ë§Œí•œ ë¯¸ì…˜ 2ê°€ì§€ ì¶”ì²œí•´ì¤˜. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì§§ê²Œ ë§í•´ì¤˜."

#     headers = {
#         "Authorization": f"Bearer {GROQ_API_KEY}",
#         "Content-Type": "application/json"
#     }

#     body = {
#         "model": "llama3-8b-8192",
#         "messages": [
#             {"role": "user", "content": prompt}
#         ],
#         "temperature": 0.7
#     }

#     try:
#         response = requests.post(GROQ_API_URL, headers=headers, json=body)
#         response.raise_for_status()
#         result = response.json()
#         message = result["choices"][0]["message"]["content"]
#         return {"message": message}
#     except Exception as e:
#         return JSONResponse(status_code=500, content={"error": str(e)})

# # ì•„ë˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
# from langchain_chroma import Chroma
# from langchain_ollama import OllamaEmbeddings

# embedding = OllamaEmbeddings(base_url="http://ollama:11434", model="llama3")
# db = Chroma(persist_directory="/chroma/chroma", embedding_function=embedding)

# @app.get("/documents")
# async def get_documents():
#     try:
#         data = db.get()
#         documents_info = []
#         for i in range(len(data['ids'])):
#             doc = {
#                 "id": data['ids'][i],
#                 "document": data['documents'][i],
#                 "metadata": data['metadatas'][i]
#             }
#             documents_info.append(doc)
#         return JSONResponse(content={"documents": documents_info})
#     except Exception as e:
#         return JSONResponse(status_code=500, content={"error": str(e)})

# app.mount("/static", StaticFiles(directory="static"), name="static")

# @app.get("/")
# def serve_index():
#     return FileResponse("static/index.html")

# from fastapi import FastAPI
# from fastapi.responses import JSONResponse, FileResponse
# from fastapi.staticfiles import StaticFiles
# from pydantic import BaseModel
# import os
# import requests
# from dotenv import load_dotenv
# import time

# # âœ… .env íŒŒì¼ ë¡œë“œ
# load_dotenv()

# # âœ… Groq API ì„¤ì •
# GROQ_API_KEY = os.getenv("GROQ_API_KEY")
# GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"

# # âœ… FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# app = FastAPI()

# # âœ… Pydantic ëª¨ë¸
# class RAGRequest(BaseModel):
#     category: str

# # âœ… Chroma (ë¬¸ì„œ ê²€ìƒ‰ìš©)
# from langchain_chroma import Chroma
# from langchain_ollama import OllamaEmbeddings

# # âš ï¸ OllamaëŠ” ì„ë² ë”©ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©ë¨ (ì„œë²„ í•„ìš”)
# embedding = OllamaEmbeddings(base_url="http://ollama:11434", model="llama3")
# db = Chroma(persist_directory="/chroma/chroma", embedding_function=embedding)

# # âœ… ì¶”ì²œ API (RAG êµ¬ì¡° ì ìš©)

# @app.post("/recommend")
# async def recommend(req: RAGRequest):
#     start_time = time.time()
#     query = f"{req.category} ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨ëœ ì˜¤ëŠ˜ í•´ë³¼ ë§Œí•œ ë¯¸ì…˜ 2ê°€ì§€ ì¶”ì²œí•´ì¤˜. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì§§ê²Œ ë§í•´ì¤˜."

#     # 1. ë¬¸ì„œ ê²€ìƒ‰
#     docs_with_scores = db.similarity_search_with_score(query, k=4)

#     for doc, score in docs_with_scores:
#         print(f"ë¬¸ì„œ ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f} / ë¬¸ì„œ: {doc.page_content[:30]}...")


#     # 2. ìœ ì‚¬ë„ í•„í„°ë§ (ì ìˆ˜ ë‚®ì„ìˆ˜ë¡ ê´€ë ¨ ìˆìŒ)
#     filtered_docs = [doc for doc, score in docs_with_scores if score < 0.53]
#     context = "\n\n".join([doc.page_content for doc in filtered_docs])

#     # 3. ë¬¸ì„œê°€ ì¶©ë¶„í•˜ë©´ RAG, ì•„ë‹ˆë©´ Groq ë‹¨ë…
#     is_rag = len(filtered_docs) >= 1 #and len(context) > 50

#     if is_rag:
#         final_prompt = (
#             f"ë‹¤ìŒì€ ì°¸ê³  ë¬¸ì„œì…ë‹ˆë‹¤:\n\n{context}\n\n"
#             f"ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ì§§ê²Œ ì¶”ì²œí•´ì£¼ì„¸ìš”:\n\n{query}"
#         )
#     else:
#         final_prompt = query

#     # 4. Groq API í˜¸ì¶œ
#     headers = {
#         "Authorization": f"Bearer {GROQ_API_KEY}",
#         "Content-Type": "application/json"
#     }

#     body = {
#         "model": "llama3-8b-8192",
#         "messages": [{"role": "user", "content": final_prompt}],
#         "temperature": 0.7
#     }

#     try:
#         response = requests.post(GROQ_API_URL, headers=headers, json=body)
#         response.raise_for_status()
#         result = response.json()
#         message = result["choices"][0]["message"]["content"]
#         end_time = time.time()  # â±ï¸ ë ì‹œê°
#         elapsed_time = round(end_time - start_time, 2)  # ì†Œìˆ˜ 2ìë¦¬ê¹Œì§€
#         return {"message": message, "response_time_sec": elapsed_time}

#     except Exception as e:
#         return JSONResponse(status_code=500, content={"error": str(e)})

# RAGë§Œ ì‚¬ìš©ìš©

# @app.post("/recommend")
# async def recommend(req: RAGRequest):
#     start_time = time.time()
#     query = f"{req.category} ì˜¤ëŠ˜ í•´ë³¼ ë§Œí•œ ë¯¸ì…˜ 2ê°€ì§€ ì¶”ì²œí•´ì¤˜. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì§§ê²Œ ë§í•´ì¤˜."

#     headers = {
#         "Authorization": f"Bearer {GROQ_API_KEY}",
#         "Content-Type": "application/json"
#     }

#     body = {
#         "model": "llama3-8b-8192",
#         "messages": [{"role": "user", "content": query}],
#         "temperature": 0.7
#     }

#     try:
#         response = requests.post(GROQ_API_URL, headers=headers, json=body)
#         result = response.json()
#         message = result["choices"][0]["message"]["content"]
#         end_time = time.time()  # â±ï¸ ë ì‹œê°
#         elapsed_time = round(end_time - start_time, 2)  # ì†Œìˆ˜ 2ìë¦¬ê¹Œì§€
#         return {"message": message, "response_time_sec": elapsed_time}
#     except Exception as e:
#         return JSONResponse(status_code=500, content={"error": str(e)})

# CoT

# @app.post("/recommend")
# async def recommend(req: RAGRequest):
#     start_time = time.time()
#     user_input = f"{req.category} ì˜¤ëŠ˜ í•´ë³¼ ë§Œí•œ ë¯¸ì…˜ 2ê°€ì§€ ì¶”ì²œí•´ì¤˜."

#     cot_prompt = (
#         "ë„ˆëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•´ ë¯¸ì…˜ì„ ì¶”ì²œí•˜ëŠ” AIì•¼. "
#         "ë¨¼ì € ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ì„í•˜ê³ , ê·¸ ì¹´í…Œê³ ë¦¬ì˜ íš¨ê³¼ë‚˜ íŠ¹ì§•ì„ í•œ ì¤„ë¡œ ìš”ì•½í•œ í›„, "
#         "ê·¸ì— ë§ëŠ” ë¯¸ì…˜ì„ 2ê°€ì§€ ì¶”ì²œí•´ì£¼ê³  ê·¸ê²Œ ì™œ ì¹´í…Œê³ ë¦¬ì˜ íš¨ê³¼ë‚˜ íŠ¹ì§•ê³¼ ë§ëŠ”ì§€ ê·¼ê±°ë¥¼ ë§í•´ì¤˜. ë°˜ë“œì‹œ ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ê³  ë§ˆì§€ë§‰ì— ìµœì¢… ê²°ê³¼ë¥¼ ì •ë¦¬í•´ì„œ í•œêµ­ì–´ë¡œ ë§í•´ì¤˜. \n\n"
#         f"ì‚¬ìš©ì ìš”ì²­: {user_input}"
#     )

#     headers = {
#         "Authorization": f"Bearer {GROQ_API_KEY}",
#         "Content-Type": "application/json"
#     }

#     body = {
#         "model": "llama3-8b-8192",
#         "messages": [{"role": "user", "content": cot_prompt}],
#         "temperature": 0.7
#     }

#     try:
#         response = requests.post(GROQ_API_URL, headers=headers, json=body)
#         result = response.json()
#         message = result["choices"][0]["message"]["content"]
#         end_time = time.time()  # â±ï¸ ë ì‹œê°
#         elapsed_time = round(end_time - start_time, 2)  # ì†Œìˆ˜ 2ìë¦¬ê¹Œì§€
#         return {"message": message, "response_time_sec": elapsed_time}
#     except Exception as e:
#         return JSONResponse(status_code=500, content={"error": str(e)})


from fastapi import FastAPI
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from dotenv import load_dotenv
import os, requests, re, json, time
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse


# í™˜ê²½ ì„¤ì •
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"

# FastAPI
app = FastAPI()

# ì„ë² ë”© & ë²¡í„° DB
embedding = OllamaEmbeddings(base_url="http://ollama:11434", model="llama3")
db = Chroma(persist_directory="/chroma/chroma", embedding_function=embedding)

class ChatRequest(BaseModel):
    category: str

@app.post("/recommend")
async def recommend(req: ChatRequest):
    start_time = time.time()
    query = f"{req.category} ì˜¤ëŠ˜ í•´ë³¼ ë§Œí•œ ë¯¸ì…˜ 2ê°€ì§€ ì¶”ì²œí•´ì¤˜."

    # ğŸ” ë¬¸ì„œ ê²€ìƒ‰
    docs_with_scores = db.similarity_search_with_score(query, k=4)
    filtered_docs = [doc for doc, score in docs_with_scores if score < 0.53]
    context = "\n\n".join([doc.page_content for doc in filtered_docs])

    # ğŸ’¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
    if len(filtered_docs) >= 1:
        # âœ… RAG ê¸°ë°˜ í”„ë¡¬í”„íŠ¸
        final_prompt = (
            "ë„ˆëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¯¸ì…˜ì„ ì¶”ì²œí•˜ëŠ” AIì•¼.\n"
            "ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´. ì„¤ëª…ì€ ìì—°ì–´ë¡œ í•˜ê³ , JSONë§Œ í¬í•¨í•´ì¤˜.\n"
            '{\n'
            '  "message": "2ê°€ì§€ ì¶”ì²œì„ ìì—°ì–´ë¡œ ì„¤ëª…",\n'
            '  "category": "í•´ë‹¹ ì¶”ì²œì´ ì†í•˜ëŠ” ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ìš´ë™, ê°ì •ê´€ë¦¬, ìê¸°ê´€ë¦¬, ì§‘ì¤‘ ë“±)"\n'
            "}\n\n"
            f"ì°¸ê³  ë¬¸ì„œ:\n{context}\n\n"
            f"ì‚¬ìš©ì ìš”ì²­: {query}"
        )
    else:
        # âœ… CoT í”„ë¡¬í”„íŠ¸
        final_prompt = (
            "ë„ˆëŠ” ë¯¸ì…˜ ì¶”ì²œ AIì•¼. ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´. ì„¤ëª…í•˜ì§€ ë§ˆ.\n"
            '{\n'
            '  "message": "2ê°€ì§€ ì¶”ì²œì„ ìì—°ì–´ë¡œ ì„¤ëª…",\n'
            '  "category": "í•´ë‹¹ ì¶”ì²œì´ ì†í•˜ëŠ” ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ìš´ë™, ê°ì •ê´€ë¦¬, ìê¸°ê´€ë¦¬, ì§‘ì¤‘ ë“±)"\n'
            "}\n\n"
            f"ì‚¬ìš©ì ìš”ì²­: {query}"
        )

    # ğŸš€ Groq API í˜¸ì¶œ
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }

    body = {
        "model": "llama3-8b-8192",
        "messages": [{"role": "user", "content": final_prompt}],
        "temperature": 0.7
    }

    try:
        response = requests.post(GROQ_API_URL, headers=headers, json=body)
        result = response.json()
        content = result["choices"][0]["message"]["content"]

        # ğŸ§  JSON íŒŒì‹±
        json_match = re.search(r"\{.*\}", content, re.DOTALL)
        if not json_match:
            raise ValueError("ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        json_str = json_match.group(0).replace("'", '"')
        parsed = json.loads(json_str)

        parsed["response_time_sec"] = round(time.time() - start_time, 2)
        return parsed

    except json.JSONDecodeError as json_err:
        return JSONResponse(status_code=500, content={
            "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
            "detail": str(json_err),
            "raw_content": content
        })
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# âœ… Chroma ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ í™•ì¸ìš©
@app.get("/documents")
async def get_documents():
    try:
        data = db.get()
        documents_info = []
        for i in range(len(data['ids'])):
            doc = {
                "id": data['ids'][i],
                "document": data['documents'][i],
                "metadata": data['metadatas'][i]
            }
            documents_info.append(doc)
        return JSONResponse(content={"documents": documents_info})
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# âœ… ì •ì  íŒŒì¼ ì„œë¹™ (HTML í¬í•¨)
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
def serve_index():
    return FileResponse("static/index.html")